---
title: "STAT206Project_RobustEM"
author: "Yuxin Liu & Yijia Xue & Chenguang Yang"
format: 
  html:
    toc: true
    toc-location: left
editor: visual
---

```{r}
# Package
library(mvtnorm)
library(Matrix)
library(ggplot2)
library(ggthemes)
```

# Define Function

## E-step

```{r}
estep <- function(X, p, mu, sigma) {
  N <- nrow(X)  # Number of samples
  K <- length(p)  # Number of components
  Z <- matrix(0,nrow=N,ncol=K) # Initiate Z
  for (k in 1:K) {
    Z[, k] <- p[k] * dmvnorm(X, mean = mu[k, ], sigma = as.matrix(sigma[[k]]))
  }
  row_sums <- rowSums(Z)
  Z <- Z / row_sums
  Z
}
```

## Robust EM Algorithm
```{r}
Robust_EM<-function(X, tol = 1e-4){
  
  # Step 1 - Initiate beta, c, p, mu
  n<-nrow(X)  # Number of samples
  P<-ncol(X) # Number of variables
  beta<-1
  c<-n # Number of clusters
  p<-rep(1/n,n)
  mu<-X # each row of X will be a mu
  
  # Step 2 - Initiate Sigma
  dist_matrix<-matrix(0,nrow=n,ncol=n)
  for (k in 1:n) {
    for (i in 1:n) {
      dist_matrix[k,i]<-sqrt(sum((X[i,]-mu[k,])^2))
    }
  }
  dist_matrix<-t(apply(dist_matrix,1,sort))
  dist_matrix<-dist_matrix[,2:n]
  Q<-min(dist_matrix)*diag(x=1,nrow=P)
  gamma<-0.0001
  sigma <- lapply(1:n, function(i) (1-gamma)*dist_matrix[i,ceiling(sqrt(c))]*diag(x=1,nrow=P)+gamma*Q)
  
  # Step 3 - Compute Z
  Z <- matrix(0,nrow=n,ncol=n)
  Z <- estep(X, p, mu, sigma)
  iteration <- 1
  
  # Step 4 - Compute mu
  for (k in 1:n) {
      mu[k,] <- colSums(Z[, k] * X) / sum(Z[, k])
  }
  
  # Start iteration
  for (i in 1:1000) {
    
    # Step 5 - Update p
    options(digits = 22)
    p_prev <- p
    p_em <- colMeans(Z)
    E <- sum(p * log(p))
    p <- p_em  + beta * p * (log(p) - E)
    
    # Step 6 - Compute beta
    eta <- min(1, 0.5^floor(P/2-1))
    beta <- min(sum(exp(-eta*n*abs(p-p_prev)))/c, (1-max(p_em))/(-max(p_prev)*E))
    
    # Step 7 - Update cluster - Update c and adjust p, Z, mu
    index <- which(p>1/n) # Discard cluster with p<=1/n
    c <- length(index)
    p <- p[index]
    p <- p/sum(p)
    Z <- Z[,index]
    Z <- Z/rowMeans(Z)
    mu<-mu[index,]
    mu_prev <- mu
    if(iteration>=60){
      if((c-n)==0){
        beta<-0
      }
    }
    
    # Step 8 - Update Sigma
    sigma <- lapply(1:ncol(Z), function(k) {
        mu_diff <- t(t(X) - mu[k, ])
        cov_matrix <- t(Z[, k] * mu_diff) %*% mu_diff / sum(Z[, k])
        nearPD(cov_matrix)$mat # Ensure positive definiteness
      })
    
    # Step 9 - Update Z
    Z <- estep(X, p, mu, sigma)
    
    # Step 10 - Update mu
    for (k in 1:ncol(Z)) {
        mu[k,] <- colSums(Z[, k] * X) / sum(Z[, k])
    }
    
    # Step 11 - Check convergence - Compare mu_prev and mu
    if(max(apply(mu_prev - mu, 1, function(row) sqrt(sum(row^2))))<tol){
      break
    }else{
      iteration<-iteration+1
    }
  }
  list(p = p, mu = mu, sigma = sigma, iteration = iteration)
}
```

## EM Algorithm

```{r}
# M-step: Update the parameters
mstep <- function(X, Z) {
  N <- nrow(X)
  K <- ncol(Z)
  p <- colMeans(Z)
  mu <- matrix(0, nrow = K, ncol = 2)
  for (k in 1:K) {
    mu[k,] <- colSums(Z[, k] * X) / sum(Z[, k])
  }
  sigma <- lapply(1:K, function(k) {
    mu_diff <- t(t(X) - mu[k, ])
    cov_matrix <- t(Z[, k] * mu_diff) %*% mu_diff / sum(Z[, k])
    nearPD(cov_matrix)$mat
  })
  list(p = p, mu = mu, sigma = sigma)
}

EM <- function(X, max_iter = 1000, tol = 1e-6, K) {
  N <- nrow(X)

  # Initialize the parameters
  p <- rep(1/K, K)  # Mixing proportions
  mu <- matrix(runif(2 * K), ncol = 2)  # Means
  sigma <- lapply(1:K, function(k) diag(runif(2)))  # Covariance matrices
  iteration<-0
    
  # Iterations
  for (iter in 1:max_iter) {
    
    # E-step
    iteration<-iteration+1
    Z <- estep(X, p, mu, sigma)
    
    # M-step
    prev_params <- list(p, mu, sigma)
    params <- mstep(X, Z)
    for (i in 1:K) {
      params[[3]][[i]]<-as.matrix(params[[3]][[i]])
    }
    p <- params[[1]]
    mu <- params[[2]]
    sigma <- params[[3]]
    
    # Check convergence
    if (max(abs(unlist(params) - unlist(prev_params))) < tol) {
      break
    }
  }
  
  # Return the estimated parameters
  list(p = p, mu = mu, sigma = sigma, iteration = iteration)
}
```

## Function to create clustering results figures

```{r}
draw_ellipse <- function(mu, sigma, level = 0.95, n_points) {
  eigen_decomp <- eigen(sigma)
  angles <- seq(0, 2 * pi, length.out = n_points)
  ellipse_points <- sqrt(qchisq(level, df = 2)) * t(eigen_decomp$vectors %*% 
                    diag(sqrt(eigen_decomp$values)) %*% rbind(cos(angles), sin(angles)))
  ellipse_points <- t(t(ellipse_points) + mu)
  ellipse_points
}

draw_cluster <-function(X,mu,sigma,iteration){
  n<-nrow(X)
  dat1<-data.frame(X)
  dat2<-data.frame(mu)
  cluster_plot <- ggplot(dat1,mapping = aes(X1,X2))+
    geom_point(size=.8,col="red")+
    geom_point(data=dat2,mapping = aes(X1,X2),size=1.2)+
    theme_base()+
    xlab("")+
    ylab("")+
    labs(title = paste("Iteration = ",iteration,"; C = ", nrow(mu)))
  for (i in 1:nrow(mu)) {
    dat3<-draw_ellipse(mu[i,],sigma[[i]],n_points=n)
    cluster_plot<-cluster_plot+
      geom_path(dat=data.frame(dat3),mapping = aes(X1,X2),col="blue",linewidth=.6)
  }
  cluster_plot
  
}
```

# Simulation 1

## Data generation

Two-dimensional, two-component Gaussian mixture distribution with sample size $n=800$.

```{r}
set.seed(111)
p1 <- c(0.5, 0.5)  
mu1 <- list(
  matrix(c(0, 0), nrow = 2),  
  matrix(c(20, 0), nrow = 2)
)
sigma1 <- list(
  matrix(c(1, 0, 0, 1), nrow = 2), 
  matrix(c(9, 0, 0, 9), nrow = 2)
)

N1 <- 800
J1 <- sample(1:length(p1), N1, replace = TRUE, prob = p1) 
X1 <- matrix(NA, nrow = N1, ncol = 2)  
Z1 <- matrix(0, nrow = N1, ncol = length(p1))

for (i in 1:N1) {
  j <- J1[i]  
  mu_i <- mu1[[j]] 
  sigma_i <- sigma1[[j]] 
  X1[i,] <- rmvnorm(1, mean = mu_i, sigma = sigma_i)
  Z1[i,j] <- 1
}
```

## Estimate parameters

```{r}
result1 <- Robust_EM(X1)
result1
```

## Clustering Results

```{r}
draw_cluster(X1, result1$mu, result1$sigma, result1$iteration)
```

## Compare with EM Algorithm

```{r}
result1.1<-EM(X1,K=2)
draw_cluster(X1, result1.1$mu, result1.1$sigma, result1.1$iteration)
```


# Simulation 2

## Data generation

Two-dimensional, three-component Gaussian mixture distribution with sample size $n=300$.

```{r}
set.seed(222)
p2 <- c(1/3, 1/3, 1/3)  
mu2 <- list(
  matrix(c(0, 3), nrow = 2),  
  matrix(c(0, 5), nrow = 2),
  matrix(c(0, 7), nrow = 2)
)
sigma2 <- list(
  matrix(c(1.2, 0, 0, 0.01), nrow = 2), 
  matrix(c(1.2, 0, 0, 0.01), nrow = 2),
  matrix(c(1.2, 0, 0, 0.01), nrow = 2)
)

N2 <- 300
J2 <- sample(1:length(p2), N2, replace = TRUE, prob = p2) 
X2 <- matrix(NA, nrow = N2, ncol = 2)  
Z2 <- matrix(0, nrow = N2, ncol = length(p2))

for (i in 1:N2) {
  j <- J2[i]  
  mu_i <- mu2[[j]] 
  sigma_i <- sigma2[[j]] 
  X2[i,] <- rmvnorm(1, mean = mu_i, sigma = sigma_i)
  Z2[i,j] <- 1
}
```

## Estimate parameters

```{r}
result2 <- Robust_EM(X2)
result2
```

## Clustering Results

```{r}
draw_cluster(X2, result2$mu, result2$sigma, result2$iteration)
```

## Compare with EM Algorithm

```{r}
result2.1<-EM(X2,K=3)
draw_cluster(X2, result2.1$mu, result2.1$sigma, result2.1$iteration)
```

# Simulation 3

## Data generation

Two-dimensional, five-component Gaussian mixture distribution with sample size $n=1000$.

```{r}
set.seed(33)
p3 <- rep(0.2,5)
mu3 <- list(
  matrix(c(0, 0), nrow = 2),  
  matrix(c(0, 0), nrow = 2),
  matrix(c(-1.5, 1.5), nrow = 2),
  matrix(c(1.5, 1.5), nrow = 2),
  matrix(c(0, -2), nrow = 2)
)
sigma3 <- list(
  matrix(c(0.01, 0, 0, 1.25), nrow = 2), 
  matrix(c(8, 0, 0, 8), nrow = 2),
  matrix(c(0.2, 0, 0, 0.015), nrow = 2),
  matrix(c(0.2, 0, 0, 0.015), nrow = 2),
  matrix(c(1, 0, 0, 0.2), nrow = 2)
)

N3 <- 1000
J3 <- sample(1:length(p3), N3, replace = TRUE, prob = p3) 
X3 <- matrix(NA, nrow = N3, ncol = 2)  
Z3 <- matrix(0, nrow = N3, ncol = length(p3))

for (i in 1:N3) {
  j <- J3[i]  
  mu_i <- mu3[[j]] 
  sigma_i <- sigma3[[j]] 
  X3[i,] <- rmvnorm(1, mean = mu_i, sigma = sigma_i)
  Z3[i,j] <- 1
}
```

## Estimate parameters

```{r}
result3 <- Robust_EM(X3)
result3
```

## Clustering Results

```{r}
draw_cluster(X3, result3$mu, result3$sigma, result3$iteration)
```

## Compare with EM Algorithm

```{r}
result3.1<-EM(X3,K=5)
draw_cluster(X3, result3.1$mu, result3.1$sigma, result3.1$iteration)
```

# Simulation 4

## Data generation

Two-dimensional, Sixteen-component Gaussian mixture distribution with sample size $n=16*50=800$.

```{r}
set.seed(444)
p4 <- rep(1/16,16)
grid_x <- c(0, 2, 4, 6)
grid_y <- c(1, 3, 5, 7)
grid <- expand.grid(grid_x, grid_y)
mu4 <- lapply(1:nrow(grid), function(i) matrix(c(grid[i, 1], grid[i, 2]), nrow = 2))

sigma4_entry <- matrix(c(0.1, 0, 0, 0.1), nrow = 2)
sigma4 <- replicate(16, sigma4_entry, simplify = FALSE)

N4 <- 16*50
J4 <- sample(1:length(p4), N4, replace = TRUE, prob = p4) 
X4 <- matrix(NA, nrow = N4, ncol = 2)  
Z4 <- matrix(0, nrow = N4, ncol = length(p4))

for (i in 1:N4) {
  j <- J4[i]  
  mu_i <- mu4[[j]] 
  sigma_i <- sigma4[[j]] 
  X4[i,] <- rmvnorm(1, mean = mu_i, sigma = sigma_i)
  Z4[i,j] <- 1
}
```

## Estimate parameters

```{r}
result4 <- Robust_EM(X4)
result4
```

## Clustering Results

```{r}
draw_cluster(X4, result4$mu, result4$sigma, result4$iteration)
```

## Compare with EM Algorithm

```{r}
result4.1<-EM(X4,K=16)
draw_cluster(X4, result4.1$mu, result4.1$sigma, result4.1$iteration)
```
